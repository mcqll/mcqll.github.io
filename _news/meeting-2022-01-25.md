---
layout: post
title: Lab meeting - Jacob Louis Hoover
date: 2022-01-25
published: true
inline: false
---

At this week's lab meeting, [**Jacob Louis Hoover**](/people/hoover.jacob.louis) will give a talk titled 'Processing time is a superlinear function of surprisal.'

- **{{ page.date | date: '%A, %B %-d' }}**, **15:00â€“16:00** (Montreal time, UTC-5).
- Meetings are via Zoom. If you would like to attend the talk but have not yet signed up for the MCQLL meetings this semester, please send an email to [mcqllmeetings@gmail.com](mailto:mcqllmeetings@gmail.com).

#### Abstract

<blockquote>
The incremental processing difficulty of a linguistic item is related to its
predictability. Surprisal theory (Hale, 2001; Levy, 2008) posits that the
processing cost of a word in context is a linear function of its surprisal.
This prediction has received considerable attention and broad support from
empirical studies using a variety of language models to estimate surprisal.
However, no algorithmic theory of processing has been proposed which scales
linearly in surprisal. Additionally, recent empirical work has also begun
raise questions about the assumption of linearity.  We present a study
specifically aimed at discerning the general shape of the linking function,
using a collection of modern pretrained language models (LMs) to estimate
surprisal. We find evidence of a superlinear effect on reading time. We also
find that the better a language model's predictions are on average, the more
clearly the relationship is between surprisal and processing is superlinear.
These results suggest revising the linearity hypothesis of surprisal theory,
and can provide support for algorithmic theories of human language processing
which scale faster than linearly in surprisal.
</blockquote>

