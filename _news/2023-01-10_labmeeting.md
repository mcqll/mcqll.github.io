---
layout: post
title: Lab meeting - Xuanda Chen and Amanda Doucette
date: 2023-01-10
published: true
inline: false
---

Welcome back to the Winter semester of MCQLL meetings! MCQLL will be meeting every other Tuesday at 15:00, same as last semester.

> __When:__ 
> : {{ page.date | date: '%A, %B %-d' }}, 15:00--16:00 (Montréal time, UTC-4)
>
> __Where:__  
> : MCQLL meetings this semester are in hybrid format.  We will meet in-person in room 117 of the McGill Linguistics Department, [1085 Dr-Penfield](https://maps.mcgill.ca/?cmp=1&txt=EN&id=Penfield1085). If you'd like to attend virtually, Zoom meetings will be held [here](https://mcgill.zoom.us/j/84089215248?pwd=UkpMK1FEV2dTaVpGSDMzLzJtNWFhUT09).

For our first meeting this semester, we will have two presenters. **[Amanda Doucette](/people/doucette.amanda)** will be presenting _A universal tendency towards vowel harmony and consonant anti-harmony_, and **[Xuanda Chen](/people/chen.xuanda)** will be presenting _When does word order matter and when doesn’t it?_ Abstracts of both talks follow.

All are welcome to attend.


-  __Speaker:__
    : Xuanda Chen

    __Title:__
    : When does word order matter and when doesn’t it?

    __Abstract:__
    : > Language models are found to be insensitive to word order perturbation when doing NLU tasks. This casts doubt on if language models are compositional, and have grammar-dependent generalizations as humans do. We think this question can be interpreted from two perspectives: representation and behavior. Language models can encode word order into contextualized representations, but they do not necessarily use it when doing tasks. The question is thus reframed as, to what extent can word order be encoded into representations? And do language models use it when doing NLU tasks? Under the information-theoretic framework, we define the word order information as the mutual information between sentences and their perturbations. We then use the usable V-information (Xu, 2020) to capture how much word order, beyond lexical items, is actually exploited in doing a specific NLU task. Our results suggest that some sentences contain less word order information than others, and language models use different amounts of word order information depending on the task. It might shed some insights into the word order puzzle: language models produce contextualized representations, but these NLU tasks do not require real understanding of meaning where word order is indispensable.

-  __Speaker:__
    : Amanda Doucette

    __Title:__
    : A universal tendency towards vowel harmony and consonant anti-harmony

    __Abstract:__
    : > Certain phonotactic constraints on the co-occurrence of segments, such as vowel harmony and consonant anti-harmony, appear to be much more common across the world's languages than others. Vowel anti-harmony and consonant harmony, for example, appear to be exceedingly uncommon. These tendencies suggest that vowels and consonants interact within words in opposite ways -- similar vowel pairs tend to co-occur, while similar consonant pairs tend to be avoided. However, evidence of this so far only comes from studies of individual languages or families. Using Bayesian negative binomial regression, we investigate patterns of co-occurrence in vowels and consonants in 107 languages across 21 families and find that the differences between consonant and vowel effects identified in the literature are near-universals across languages. Furthermore, we find evidence that consonant co-occurrence effects are consistently larger than vowel co-occurrence effects, and weak evidence of a trade-off between vowel and consonant co-occurrence effects.
