---
layout: post
title: Lab meeting - Jacob Louis Hoover
date: 2020-10-21
published: true
inline: false 
---

At this week's lab meeting, [**Jacob**](/people/hoover.jacob.louis) will be presenting on the connection between grammatical structure and the statistics of word occurrences in language use.

- **{{ page.date | date: '%A, %B %-d' }}**, at **13:30** (Montr√©al time, UTC-4).
- Meetings are via Zoom. If you would like to attend and have not already signed up for the MCQLL mailing list, please fill out [this google form](https://forms.gle/fBu5eYfiF2Ctnv5e7).

#### Abstract

<blockquote>
	There is an intuitive connection between grammatical structure and the statistics of word occurrences observed in language use. This intuitive connection is reflected in cognitive models and also in NLP, in the assumption that the patterns of predictability correlate with linguistic structure. We will call this general idea the *dependency-dependence* hypothesis. This hypothesis is implicit in the use of language modelling objectives for training modern neural models, and has been made explicitly in some approaches to unsupervised dependency parsing. The strongest version of this hypothesis is to say that compositional structure is in fact entirely reducible to cooccurrence statistics (a hypothesis made explicit in Futrell et al. <a href="http://dx.doi.org/10.18653/v1/W19-7703">2019</a>). In this talk I will describe a study using the mutual information of pairs of words using pretrained contextualized embedding models to show that the optimal structure for prediction is not very closely correlated to the compositional structure. In this work we propose that contextualized mutual information scores of this kind may be useful as a way to understand the structure of predictability, as a system distinct from compositional structure, but also integral to language use.
</blockquote>

#### Bio
 
Jacob is a PhD student at McGill Linguistics / Mila. He is broadly interested in logic, mathematical linguistics, and the generative / expressive capacity of formal systems, as well as information theory, and examining what both human and machine learning might be able to tell us about the underlying structure of language.
