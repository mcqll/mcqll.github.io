<!DOCTYPE html>
<html>

  <head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width initial-scale=1" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge">

  <title>MCQLL
     | Lab meeting - Austin Kraft and Amirhossein Kazemnejad
  </title>
  <meta name="description" content="Montréal Computational & Quantitative Linguistics Lab
">

  <link rel="shortcut icon" href="/assets/img/favicon.ico">

  <link rel="stylesheet" href="/assets/css/main.css">
  <link rel="canonical" href="/news/2023-12-05_labmeeting/">
</head>


  <body>

    <header class="site-header">

  <div class="wrapper">

    
    <span class="site-title">
        <a class="page-link" href="/">
          <strong><div class="logodiv">
              <img class="logoimg" src="/assets/img/mcqll-logo-transparent.png">
          </div></strong>
        </a>
    </span>
    

    <nav class="site-nav">
      <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path fill="#424242" d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.031C17.335,0,18,0.665,18,1.484L18,1.484z"/>
              <path fill="#424242" d="M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0c0-0.82,0.665-1.484,1.484-1.484 h15.031C17.335,6.031,18,6.696,18,7.516L18,7.516z"/>
              <path fill="#424242" d="M18,13.516C18,14.335,17.335,15,16.516,15H1.484C0.665,15,0,14.335,0,13.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.031C17.335,12.031,18,12.696,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

      <div class="trigger">

        <!-- Blog -->
        <!-- <a class="page-link" href="/blog/">blog</a> -->

        <!-- Pages -->
        
          
            <a class="page-link" href="/about/">About</a>
          
        
          
        
          
            <a class="page-link" href="/join">Join</a>
          
        
          
        
          
            <a class="page-link" href="/meetings/">Meetings</a>
          
        
          
            <a class="page-link" href="/people">People</a>
          
        
          
            <a class="page-link" href="/publications">Publications</a>
          
        
          
            <a class="page-link" href="/resources">Resources</a>
          
        

      </div>
    </nav>

  </div>

</header>



    <div class="page-content">
      <div class="wrapper">
        <div class="post">

  <header class="post-header">
    <h1 class="post-title">
      
      Lab meeting - Austin Kraft and Amirhossein Kazemnejad
    </h1>
    
    <p class="post-meta">December 5, 2023</p>
  </header>

  <article class="post-content">
    <p>At this week’s MCQLL meeting, <a href="https://austinwkraft.github.io/about/"><strong>Austin Kraft</strong></a> will
be presenting <strong>[[Small language] corpus] problems</strong>, and <a href="https://mila.quebec/en/person/amirhossein-kazemnejad/"><strong>Amirhossein Kazemnejad</strong></a> will
be presenting <strong>Enhancing Length Generalization in Transformers: The Role of Positional Encoding</strong>.</p>

<p><em>This is the last lab meeting of the Fall semester.  See you in Winter 2024!</em></p>

<blockquote>
  <dl>
    <dt><strong>When:</strong></dt>
    <dd>Tuesday, December 5, 15:00–16:00 (Montréal time, UTC-5)</dd>
    <dt><strong>Where:</strong></dt>
    <dd>MCQLL meetings this semester are in hybrid format.  We will meet in-person
in room 117 of the McGill Linguistics Department, <a href="https://maps.mcgill.ca/?cmp=1&amp;txt=EN&amp;id=Penfield1085">1085
Dr-Penfield</a>. If you’d
like to attend virtually, the Zoom link is
<a href="https://mcgill.zoom.us/j/85321158610">here</a>.</dd>
  </dl>
</blockquote>

<p>All are welcome to attend.</p>

<ul>
  <li>
    <dl>
      <dt><strong>Speaker:</strong></dt>
      <dd>Austin Kraft</dd>
      <dt><strong>Title:</strong></dt>
      <dd>[[Small language] corpus] problems</dd>
      <dt><strong>Abstract:</strong></dt>
      <dd>
        <blockquote>
          <p>This presentation describes an in-progress collaboration to build a corpus of Semarangan Javanese (a.k.a. Peranakan Javanese; Cole et al. 2007) of Semarang, Central Java, Indonesia. In pursuing the empirical focus of the project—documenting anaphoric expressions like reflexive pronouns—the project has brought to the fore nontrivial design choices about how to represent a language that is underdocumented in corpora (Anand, Chung &amp; Wagers 2020). As a case study from the Semarangan Javanese project, I discuss how existing part-of-speech tagsets might not neatly map to part-of-speech categories in the language, with consequences for how grammatical structure can be encoded in and retrieved from the corpus.</p>

          <p>I situate these challenges within an early-stage typology of “small language corpus problems”: conceptual and technological issues that are made acutely relevant when creating a corpus for a small language. I compare the design needs of corpora for small languages with those of small corpora for extensively documented languages, such as a corpus of English dedicated solely to a niche discourse type or subject matter (Koester 2022). Alongside the Semarangan Javanese project, my ongoing literature review aims to identify and categorize common challenges in corpus-building for small languages.</p>
        </blockquote>
      </dd>
    </dl>
  </li>
  <li>
    <dl>
      <dt><strong>Speaker:</strong></dt>
      <dd>Amirhossein Kazemnejad</dd>
      <dt><strong>Title:</strong></dt>
      <dd>Enhancing Length Generalization in Transformers: The Role of Positional Encoding</dd>
      <dt><strong>Abstract:</strong></dt>
      <dd>
        <blockquote>
          <p>The advent of Long-Context Language Models (LLMs) has unlocked numerous advantages, including extended in-context learning, prolonged text generation capabilities, and an increased number of conversational turns. Such advancements have only recently become feasible, largely due to engineering breakthroughs like flash attention, which allow the processing of extensive sequences within memory constraints. Traditionally, Transformer models, however, have been limited in their ability to generalize beyond the context sizes encountered during training. In our NeurIPS 2023 paper, we delve into an empirical investigation of prevalent Positional Encodings to scrutinize their impact on length extrapolation. Furthermore, we propose and examine the possibility of removing positional encoding altogether from the standard decoder-only Transformer. This exploration encompasses both theoretical and practical analyses. Our findings not only provide critical insights but also lay the groundwork for the evolution of Transformer architectures in next-generation LLMs.</p>
        </blockquote>
      </dd>
    </dl>
  </li>
</ul>

  </article>

  

</div>

      </div>
    </div>

    <!-- <footer>

  <div class="wrapper">
    &copy; 2025 MCQLL.
    
    
  </div>

</footer>
 -->

    <!-- Load jQuery -->
<script src="//code.jquery.com/jquery-1.12.4.min.js"></script>

<!-- Load Common JS -->
<script src="/assets/js/common.js"></script>

<!-- HACK, add latecx support with mathjax not sure why this isn't working, but this should do it -->
<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>





<!-- Include custom icon fonts -->
<link rel="stylesheet" href="/assets/css/fontawesome-all.min.css">
<link rel="stylesheet" href="/assets/css/academicons.min.css">

<!-- Google Analytics -->
<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', '', 'auto');
ga('send', 'pageview');
</script>


  </body>

</html>
